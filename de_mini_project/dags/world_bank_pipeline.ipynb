{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library as pd for data manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# Import BeautifulSoup from bs4 for web scraping\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import sqlite3 for working with SQLite databases\n",
    "import sqlite3\n",
    "\n",
    "# Import numpy as np for numerical operations\n",
    "import numpy as np\n",
    "\n",
    "# Import requests for making HTTP requests\n",
    "import requests\n",
    "\n",
    "# Import create_engine from sqlalchemy for database interaction\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Import countries from pycountry for country code lookup\n",
    "from pycountry import countries\n",
    "\n",
    "# Import Winsorizer from feature_engine for outlier handling\n",
    "from feature_engine.outliers import Winsorizer\n",
    "\n",
    "# Import OrdinalEncoder from sklearn for encoding categorical features\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Import StandardScaler from sklearn for feature scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Import defaultdict from collections for creating a dictionary with default values\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import datetime for working with dates and times\n",
    "from datetime import datetime\n",
    "\n",
    "# Import LocalFilesystemToGCSOperator from airflow.providers.google.cloud.transfers for transferring files from local filesystem to Google Cloud Storage\n",
    "from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator\n",
    "\n",
    "# Import DAG from airflow.models for defining Directed Acyclic Graphs in Airflow\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Import PythonOperator from airflow.operators.python for defining Python tasks in Airflow\n",
    "from airflow.operators.python import PythonOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_column_names(df, non_countries):\n",
    "    \"\"\"\n",
    "    Clean column names by trimming whitespace and replacing spaces with underscores.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - non_countries (list): List of columns that are not country names.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with cleaned column names.\n",
    "    \"\"\"\n",
    "    df.columns = [col.strip().lower().replace(' ', '_') for col in df.columns] \n",
    "    return df\n",
    "\n",
    "def convert_to_datetime(df, columns):\n",
    "    \"\"\"\n",
    "    Convert specified columns to datetime format.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - columns (list): List of columns to convert to datetime.\n",
    "\n",
    "    Returns:\n",
    "    - None: Modifies DataFrame in place.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = pd.to_datetime(df[col])\n",
    "\n",
    "def remove_commas_and_convert_to_float(df, columns):\n",
    "    \"\"\"\n",
    "    Remove commas and convert specified columns to float format.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - columns (list): List of columns to clean and convert to float.\n",
    "\n",
    "    Returns:\n",
    "    - None: Modifies DataFrame in place.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].str.replace(',', '').astype(float)\n",
    "\n",
    "def clean_string_columns(df, columns):\n",
    "    \"\"\"\n",
    "    Clean string columns by removing specified characters.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - columns (list): List of columns to clean.\n",
    "\n",
    "    Returns:\n",
    "    - None: Modifies DataFrame in place.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda x: x.split('!$!')[0] if isinstance(x, str) else x)\n",
    "\n",
    "def fill_na_with_value(df, columns, value):\n",
    "    \"\"\"\n",
    "    Fill missing values in specified columns with a specified value.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - columns (list): List of columns to fill missing values.\n",
    "    - value: Value to fill missing values with.\n",
    "\n",
    "    Returns:\n",
    "    - None: Modifies DataFrame in place.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col].fillna(value, inplace=True)\n",
    "\n",
    "def replace_nan_with_zero(df, columns):\n",
    "    \"\"\"\n",
    "    Replace NaN values in specified columns with zero.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "    - columns (list): List of columns to replace NaN values.\n",
    "\n",
    "    Returns:\n",
    "    - None: Modifies DataFrame in place.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "\n",
    "def fill_missing_values_gdp_population(gdp_df, population_df, columns_to_fill):\n",
    "    \"\"\"\n",
    "    Fill missing values in GDP and population DataFrames using backward and forward filling methods.\n",
    "\n",
    "    Args:\n",
    "    - gdp_df (DataFrame): GDP DataFrame.\n",
    "    - population_df (DataFrame): Population DataFrame.\n",
    "    - columns_to_fill (list): List of columns to fill missing values.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple of modified GDP and population DataFrames.\n",
    "    \"\"\"\n",
    "    gdp_df[columns_to_fill] = gdp_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    population_df[columns_to_fill] = population_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    return gdp_df, population_df\n",
    "\n",
    "def impute_mean_and_mode_projects(projects_df):\n",
    "    \"\"\"\n",
    "    Impute missing values in projects DataFrame with mean for numerical columns and mode for categorical columns.\n",
    "\n",
    "    Args:\n",
    "    - projects_df (DataFrame): Projects DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: Modified projects DataFrame.\n",
    "    \"\"\"\n",
    "    boardapprovaldate_mean = projects_df['boardapprovaldate'].mean()\n",
    "    closingdate_mean = projects_df['closingdate'].mean()\n",
    "    projects_df['boardapprovaldate'].fillna(boardapprovaldate_mean, inplace=True)\n",
    "    projects_df['closingdate'].fillna(closingdate_mean, inplace=True)\n",
    "\n",
    "    columns_to_fill = ['supplementprojectflg', 'countryname', 'prodline', 'lendinginstr', 'productlinetype', 'projectstatusdisplay', 'status']\n",
    "    for column in columns_to_fill:\n",
    "        most_frequent_value = projects_df[column].mode()[0]\n",
    "        projects_df[column].fillna(most_frequent_value, inplace=True)\n",
    "    return projects_df\n",
    "\n",
    "def clean_columns(dfs):\n",
    "    \"\"\"\n",
    "    Remove specified columns from DataFrames.\n",
    "\n",
    "    Args:\n",
    "    - dfs (list): List of DataFrames to clean.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of cleaned DataFrames.\n",
    "    \"\"\"\n",
    "    for df in dfs:\n",
    "        df.drop(columns=['indicator_name', 'indicator_code'], inplace=True)\n",
    "    return dfs\n",
    "\n",
    "def encode_categorical_columns(df):\n",
    "    \"\"\"\n",
    "    Encode categorical columns using one-hot encoding.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with encoded categorical columns.\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == 'object':\n",
    "            if (11 <= df[column].nunique() < 50) or (column in ['countryname', 'country_name', 'country_code', 'country_or_area', 'prodline', 'lendinginstr', 'productlinetype', 'projectstatusdisplay', 'status']):\n",
    "                df = pd.get_dummies(df, columns=[column], prefix=column)\n",
    "            else:\n",
    "                df.drop([column], axis=1, inplace=True)\n",
    "    return df.astype(int)\n",
    "\n",
    "def add_country_codes(df):\n",
    "    \"\"\"\n",
    "    Add ISO-3 country codes to DataFrame based on country names.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): Input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame with added country codes.\n",
    "    \"\"\"\n",
    "    # Manually defined country codes\n",
    "    country_codes = {\n",
    "        'Africa': 'AFR',\n",
    "        'Andean Countries': 'AND',\n",
    "        'Aral Sea': 'ARL',\n",
    "        'Asia': 'ASI',\n",
    "        'Caribbean': 'CAR',\n",
    "        'Caucasus': 'CAU',\n",
    "        'Central Africa': 'CAF',\n",
    "        'Central America': 'CAM',\n",
    "        'Central Asia': 'CAS',\n",
    "        'Co-operative Republic of Guyana': 'GUY',\n",
    "        'Commonwealth of Australia': 'AUS',\n",
    "        'Democratic Republic of Sao Tome and Prin': 'STP',\n",
    "        'Democratic Republic of the Congo': 'COD',\n",
    "        'Democratic Socialist Republic of Sri Lan': 'LKA',\n",
    "        'EU Accession Countries': 'EUA',\n",
    "        'East Asia and Pacific': 'EAP',\n",
    "        'Eastern Africa': 'EAF',\n",
    "        'Europe and Central Asia': 'ECA',\n",
    "        'Islamic  Republic of Afghanistan': 'AFG',\n",
    "        'Kingdom of Swaziland': 'SWZ',\n",
    "        'Latin America': 'LAT',\n",
    "        'Macedonia; former Yugoslav Republic of;Macedonia; former Yugoslav Republic of': 'MKD',\n",
    "        'Mekong': 'MEK',\n",
    "        'Mercosur': 'MCS',\n",
    "        'Middle East and North Africa': 'MEA',\n",
    "        'Multi-Regional': 'MRG',\n",
    "        'Organization of Eastern Caribbean States': 'ECS',\n",
    "        'Oriental Republic of Uruguay': 'URY',\n",
    "        'Pacific Islands': 'PCI',\n",
    "        'Red Sea and Gulf of Aden': 'RED',\n",
    "        'Republic of Congo': 'COG',\n",
    "        \"Republic of Cote d'Ivoire\": 'CIV',\n",
    "        'Republic of Korea': 'KOR',\n",
    "        'Republic of Kosovo': 'KOS',\n",
    "        'Republic of Niger': 'NER',\n",
    "        'Republic of Rwanda': 'RWA',\n",
    "        'Republic of Togo': 'TGO',\n",
    "        'Republic of Turkey': 'TUR',\n",
    "        'Republic of the Union of Myanmar': 'MMR',\n",
    "        'Republica Bolivariana de Venezuela': 'VEN',\n",
    "        'Sint Maarten': 'SXM',\n",
    "        'Socialist Federal Republic of Yugoslavia': 'YUG',\n",
    "        \"Socialist People's Libyan Arab Jamahiriy\": 'LBY',\n",
    "        'Socialist Republic of Vietnam': 'VNM',\n",
    "        'Somali Democratic Republic': 'SOM',\n",
    "        'South Asia': 'SAS',\n",
    "        'Southern Africa': 'SAF',\n",
    "        'St. Kitts and Nevis': 'KNA',\n",
    "        'St. Lucia': 'LCA',\n",
    "        'St. Vincent and the Grenadines': 'VCT',\n",
    "        'State of Eritrea': 'ERI',\n",
    "        'Taiwan; China;Taiwan; China': 'TWN',\n",
    "        'The Independent State of Papua New Guine': 'PNG',\n",
    "        'West Bank and Gaza': 'WBG',\n",
    "        'Western Africa': 'WAF',\n",
    "        'Western Balkans': 'WBL',\n",
    "        'World': 'WLD'\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Dictionary to store country code mappings\n",
    "    project_country_abbrev_dict = defaultdict(str)\n",
    "    country_not_found = []  # Stores countries not found in the pycountry library\n",
    "    \n",
    "    # Iterate through the country names in the DataFrame\n",
    "    for country in df['countryname'].drop_duplicates().sort_values():\n",
    "        try:\n",
    "            # Look up the country name in the pycountry library\n",
    "            # Store the country name as the dictionary key and the ISO-3 code as the value\n",
    "            project_country_abbrev_dict[country] = countries.lookup(country).alpha_3\n",
    "        except:\n",
    "            # If the country name is not found in the pycountry library, print it out and store it in the country_not_found list\n",
    "            print(country, ' not found')\n",
    "            country_not_found.append(country)\n",
    "    \n",
    "    # Update the dictionary with manually defined country codes\n",
    "    project_country_abbrev_dict.update(country_codes)\n",
    "    \n",
    "    # Map country codes to country names in the DataFrame\n",
    "    df['country_code'] = df['countryname'].apply(lambda x: project_country_abbrev_dict[x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data():\n",
    "    \"\"\"\n",
    "    Load data from various sources including CSV files, JSON files, SQLite database, XML file, and external API.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing DataFrames loaded from different sources.\n",
    "\n",
    "    Notes:\n",
    "        - CSV files are loaded using Pandas `read_csv` function.\n",
    "        - JSON files are loaded using Pandas `read_json` function.\n",
    "        - XML data is parsed using BeautifulSoup and loaded into a DataFrame.\n",
    "        - SQLite database is queried using Pandas `read_sql` function.\n",
    "        - Data from an external API (World Bank API) is fetched using the requests library and loaded into a DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> data = load_data()\n",
    "        >>> projects_df = data['projects_df']\n",
    "        >>> population_df = data['population_df']\n",
    "        >>> # Access other DataFrames similarly\n",
    "    \"\"\"\n",
    "\n",
    "    # Load XML data\n",
    "    def load_xml_data():\n",
    "        with open('/opt/airflow/data/population_data.xml', 'r') as f:\n",
    "            xml_data = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(xml_data, 'lxml')\n",
    "        data = {'country_or_area': [], 'item': [], 'year': [], 'value': []}\n",
    "\n",
    "        records = soup.find_all('record')\n",
    "\n",
    "        for record in records:\n",
    "            country_or_area = record.find('field', {'name': 'Country or Area'})\n",
    "            item = record.find('field', {'name': 'Item'})\n",
    "            year = record.find('field', {'name': 'Year'})\n",
    "            value = record.find('field', {'name': 'Value'})\n",
    "\n",
    "            if country_or_area and item and year and value:\n",
    "                data['country_or_area'].append(country_or_area.text)\n",
    "                data['item'].append(item.text)\n",
    "                data['year'].append(year.text)\n",
    "                data['value'].append(value.text)\n",
    "\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    # Load data from different sources\n",
    "    projects_df = pd.read_csv(\"/opt/airflow/data/projects_data.csv\", dtype='str')\n",
    "    population_df = pd.read_csv(\"/opt/airflow/data/population_data.csv\", skiprows=4)\n",
    "    population_json_df = pd.read_json('/opt/airflow/data/population_data.json', orient='records')\n",
    "    population_xml_df = load_xml_data()\n",
    "    conn = sqlite3.connect('/opt/airflow/data/population_data.db')\n",
    "    population_sql_df = pd.read_sql('SELECT * FROM population_data', conn)\n",
    "    rural_df = pd.read_csv('/opt/airflow/data/rural_population_percent.csv', skiprows=4)\n",
    "    electricity_df = pd.read_csv('/opt/airflow/data/electricity_access_percent.csv', skiprows=4)\n",
    "    gdp_df = pd.read_csv('/opt/airflow/data/gdp_data.csv', skiprows=4)\n",
    "    mystery_df = pd.read_csv('/opt/airflow/data/mystery.csv', encoding='utf-16')\n",
    "\n",
    "\n",
    "\n",
    "    # Fetching data from API\n",
    "    dfs = []\n",
    "    country_codes = ['ind'] #population_df['Country Code'].unique()\n",
    "\n",
    "    for country_code in country_codes:\n",
    "        api_url = f\"https://api.worldbank.org/v2/countries/{country_code}/indicators/SP.POP.TOTL/?format=json\"\n",
    "        response = requests.get(api_url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            data = json_data[1]\n",
    "            df = pd.DataFrame(data)\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for country code: {country_code}\")\n",
    "\n",
    "    population_api_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Data loading\n",
    "    population_api_df['indicator_id'] = population_api_df['indicator'].apply(lambda x: x['id'])\n",
    "    population_api_df['indicator_name'] = population_api_df['indicator'].apply(lambda x: x['value'])\n",
    "    population_api_df['country_code'] = population_api_df['countryiso3code']\n",
    "    population_api_df['year'] = population_api_df['date']\n",
    "    population_api_df['country_name'] = population_api_df['country'].apply(lambda x: x['value'])\n",
    "    population_api_df.drop(['indicator', 'country'], axis=1, inplace=True)\n",
    "    population_api_df = population_api_df[['country_name', 'country_code', 'indicator_id', 'indicator_name', 'year', 'value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "main_df = pd.concat([rural_df, electricity_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original DataFrames to keep them intact\n",
    "    projects_cleaned_df = projects_df.copy()\n",
    "    population_cleaned_df = population_df.copy()\n",
    "    population_json_cleaned_df = population_json_df.copy()\n",
    "    population_xml_cleaned_df = population_xml_df.copy()\n",
    "    population_sql_cleaned_df = population_sql_df.copy()\n",
    "    main_cleaned_df = main_df.copy()\n",
    "    mystery_cleaned_df = mystery_df.copy()\n",
    "    gdp_cleaned_df = gdp_df.copy()\n",
    "    rural_cleaned_df = rural_df.copy()\n",
    "    electricity_cleaned_df = electricity_df.copy()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    rural_cleaned_df.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "    electricity_cleaned_df.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "    projects_cleaned_df.drop(['Unnamed: 56'], axis=1, inplace=True)\n",
    "    population_sql_cleaned_df.drop(['index'], axis=1, inplace=True)\n",
    "    population_cleaned_df.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "    mystery_cleaned_df.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "    gdp_cleaned_df.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "    main_cleaned_df.drop(['Unnamed: 62'], axis=1, inplace=True)\n",
    "\n",
    "    # Define non-country values\n",
    "    non_countries = [\n",
    "        'World', 'High income', 'OECD members', 'Post-demographic dividend', 'IDA & IBRD total', 'Low & middle income',\n",
    "        'Middle income', 'IBRD only', 'East Asia & Pacific', 'Europe & Central Asia', 'North America',\n",
    "        'Upper middle income', 'Late-demographic dividend', 'European Union',\n",
    "        'East Asia & Pacific (excluding high income)', 'East Asia & Pacific (IDA & IBRD countries)', 'Euro area',\n",
    "        'Early-demographic dividend', 'Lower middle income', 'Latin America & Caribbean',\n",
    "        'Latin America & the Caribbean (IDA & IBRD countries)', 'Latin America & Caribbean (excluding high income)',\n",
    "        'Europe & Central Asia (IDA & IBRD countries)', 'Middle East & North Africa',\n",
    "        'Europe & Central Asia (excluding high income)', 'South Asia (IDA & IBRD)', 'South Asia', 'Arab World',\n",
    "        'IDA total', 'Sub-Saharan Africa', 'Sub-Saharan Africa (IDA & IBRD countries)',\n",
    "        'Sub-Saharan Africa (excluding high income)', 'Middle East & North Africa (excluding high income)',\n",
    "        'Middle East & North Africa (IDA & IBRD countries)', 'Central Europe and the Baltics',\n",
    "        'Pre-demographic dividend', 'IDA only', 'Least developed countries: UN classification', 'IDA blend',\n",
    "        'Fragile and conflict affected situations', 'Heavily indebted poor countries (HIPC)', 'Low income', 'Small states',\n",
    "        'Other small states', 'Not classified', 'Caribbean small states', 'Pacific island small states'\n",
    "    ]\n",
    "\n",
    "    # Clean and filter DataFrames\n",
    "    projects_cleaned_df = clean_column_names(projects_cleaned_df, non_countries)\n",
    "    population_cleaned_df = clean_column_names(population_cleaned_df, non_countries)\n",
    "    population_json_cleaned_df = clean_column_names(population_json_cleaned_df, non_countries)\n",
    "    population_xml_cleaned_df = clean_column_names(population_xml_cleaned_df, non_countries)\n",
    "    population_sql_cleaned_df = clean_column_names(population_sql_cleaned_df, non_countries)\n",
    "    main_cleaned_df = clean_column_names(main_cleaned_df, non_countries)\n",
    "    mystery_cleaned_df = clean_column_names(mystery_cleaned_df, non_countries)\n",
    "    gdp_cleaned_df = clean_column_names(gdp_cleaned_df, non_countries)\n",
    "    rural_cleaned_df = clean_column_names(rural_cleaned_df, non_countries)\n",
    "    electricity_cleaned_df = clean_column_names(electricity_cleaned_df, non_countries)\n",
    "\n",
    "        \n",
    "\n",
    "    # Convert columns to datetime\n",
    "    convert_to_datetime(projects_cleaned_df, ['boardapprovaldate', 'closingdate'])\n",
    "\n",
    "    # Remove commas and convert to float\n",
    "    remove_commas_and_convert_to_float(projects_cleaned_df, ['lendprojectcost', 'ibrdcommamt', 'idacommamt', 'totalamt', 'grantamt'])\n",
    "\n",
    "    # Apply lambda function to countryname column\n",
    "    projects_cleaned_df['countryname'] = projects_cleaned_df['countryname'].apply(lambda x: x.split(';')[0].strip() if (';' in x) and (x.split(';')[0].strip() == x.split(';')[1].strip()) else x)\n",
    "\n",
    "    # Clean sector and theme columns\n",
    "    clean_string_columns(projects_cleaned_df, ['sector1', 'sector2', 'sector3', 'sector4', 'sector5', 'theme1', 'theme2', 'theme3', 'theme4', 'theme5'])\n",
    "    \n",
    "    # Concatenate values of sector and theme columns\n",
    "    projects_cleaned_df['sector'] = projects_cleaned_df[['sector1', 'sector2', 'sector3', 'sector4', 'sector5']].apply(lambda row: ';'.join(row.dropna()), axis=1)\n",
    "    projects_cleaned_df['theme'] = projects_cleaned_df[['theme1', 'theme2', 'theme3', 'theme4', 'theme5']].apply(lambda row: ';'.join(row.dropna()), axis=1)\n",
    "\n",
    "    # Clean location column\n",
    "    projects_cleaned_df['country'] = projects_cleaned_df['location'].apply(lambda x: ';'.join([loc.split('!$!')[-1] for loc in x.split(';')]) if isinstance(x, str) else np.NaN)\n",
    "\n",
    "    # Apply regex replacement to string columns\n",
    "    projects_cleaned_df.replace({'^(\\(Historic\\))': ''}, regex=True, inplace=True)\n",
    "\n",
    "    # Drop columns with all NaN values\n",
    "    projects_cleaned_df.dropna(axis=1, how='all', inplace=True)\n",
    "\n",
    "    # Replace NaN values with 0 in numeric columns\n",
    "    replace_nan_with_zero(projects_cleaned_df, projects_cleaned_df.select_dtypes(include=['number']).columns)\n",
    "    replace_nan_with_zero(mystery_cleaned_df, mystery_cleaned_df.select_dtypes(include=['number']).columns)\n",
    "\n",
    "    # Fill missing values in GDP and population DataFrames using backward fill followed by forward fill\n",
    "    columns_to_fill = gdp_cleaned_df.columns[4:62]  # Assuming columns \"1960\" to \"2017\"\n",
    "    columns_to_fill_2 = electricity_cleaned_df.columns[4:62]  # Assuming columns \"1960\" to \"2017\"\n",
    "    gdp_cleaned_df[columns_to_fill] = gdp_cleaned_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    population_cleaned_df[columns_to_fill] = population_cleaned_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    population_json_cleaned_df[columns_to_fill] = population_json_cleaned_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    population_sql_cleaned_df[columns_to_fill] = population_sql_cleaned_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    rural_cleaned_df[columns_to_fill] = rural_cleaned_df[columns_to_fill].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "    electricity_cleaned_df[columns_to_fill_2] = electricity_cleaned_df[columns_to_fill_2].fillna(method='bfill', axis=1).fillna(method='ffill', axis=1)\n",
    "\n",
    "    # Mean Imputation for 'boardapprovaldate' and 'closingdate' columns in projects_cleaned_df\n",
    "    boardapprovaldate_mean = projects_cleaned_df['boardapprovaldate'].mean()\n",
    "    closingdate_mean = projects_cleaned_df['closingdate'].mean()\n",
    "    projects_cleaned_df['boardapprovaldate'].fillna(boardapprovaldate_mean, inplace=True)\n",
    "    projects_cleaned_df['closingdate'].fillna(closingdate_mean, inplace=True)\n",
    "\n",
    "    # # Remove non-country values from the data\n",
    "    population_cleaned_df = population_cleaned_df[~population_cleaned_df['country_name'].isin(non_countries)]\n",
    "    population_json_cleaned_df = population_json_cleaned_df[~population_json_cleaned_df['country_name'].isin(non_countries)]\n",
    "    population_sql_cleaned_df = population_sql_cleaned_df[~population_sql_cleaned_df['country_name'].isin(non_countries)]\n",
    "    population_xml_cleaned_df = population_xml_cleaned_df[~population_xml_cleaned_df['country_or_area'].isin(non_countries)]\n",
    "\n",
    "    # Mode Imputation for other categorical columns in projects_cleaned_df\n",
    "    columns_to_fill = ['supplementprojectflg', 'countryname', 'prodline', 'lendinginstr', 'productlinetype', 'projectstatusdisplay', 'status']\n",
    "    for column in columns_to_fill:\n",
    "        most_frequent_value = projects_cleaned_df[column].mode()[0]\n",
    "        projects_cleaned_df[column].fillna(most_frequent_value, inplace=True)\n",
    " \n",
    "\n",
    "    population_cleaned_df = population_cleaned_df.dropna(subset=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017'])\n",
    "    population_json_cleaned_df = population_cleaned_df.dropna(subset=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017'])\n",
    "    population_sql_cleaned_df = population_cleaned_df.dropna(subset=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017'])\n",
    "    gdp_cleaned_df = gdp_cleaned_df.dropna(subset=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017'])\n",
    "    rural_cleaned_df = rural_cleaned_df.dropna(subset=['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017'])\n",
    "    electricity_cleaned_df = electricity_cleaned_df.dropna(subset=['1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Types & Structure Adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    population_xml_cleaned_df['year'] = population_xml_cleaned_df['year'].astype('int')\n",
    "    population_xml_cleaned_df['value'] = population_xml_cleaned_df['value'].replace('', np.nan)\n",
    "    population_xml_cleaned_df['value'] = population_xml_cleaned_df.sort_values('year').groupby('country_or_area')['value'].fillna(method='ffill').fillna(method='bfill').astype(float)\n",
    "\n",
    "    population_cleaned_df = population_cleaned_df.drop(columns=['indicator_name', 'indicator_code'])\n",
    "    population_json_cleaned_df = population_json_cleaned_df.drop(columns=['indicator_name', 'indicator_code'])\n",
    "    population_sql_cleaned_df = population_sql_cleaned_df.drop(columns=['indicator_name', 'indicator_code'])\n",
    "    population_xml_cleaned_df = population_xml_cleaned_df.drop(columns=['item'])\n",
    "    gdp_cleaned_df = gdp_cleaned_df.drop(columns=['indicator_name', 'indicator_code'])\n",
    "    rural_cleaned_df = rural_cleaned_df.drop(columns=['indicator_name', 'indicator_code'])\n",
    "    electricity_cleaned_df = electricity_cleaned_df.drop(columns=['indicator_name', 'indicator_code'])\n",
    "\n",
    "    # Melt the DataFrame to stack the '1960' to '2017' columns into a single 'year' column\n",
    "    population_cleaned_df = population_cleaned_df.melt(id_vars=['country_name', 'country_code'], var_name='year', value_name='value')# Melt the DataFrame to stack the '1960' to '2017' columns into a single 'year' column\n",
    "    population_json_cleaned_df = population_json_cleaned_df.melt(id_vars=['country_name', 'country_code'], var_name='year', value_name='value')# Melt the DataFrame to stack the '1960' to '2017' columns into a single 'year' column\n",
    "    population_sql_cleaned_df = population_sql_cleaned_df.melt(id_vars=['country_name', 'country_code'], var_name='year', value_name='value')\n",
    "    gdp_cleaned_df = gdp_cleaned_df.melt(id_vars=['country_name', 'country_code'], var_name='year', value_name='gdp')\n",
    "    rural_cleaned_df = rural_cleaned_df.melt(id_vars=['country_name', 'country_code'], var_name='year', value_name='ruralpopulationpercent')\n",
    "    electricity_cleaned_df = electricity_cleaned_df.melt(id_vars=['country_name', 'country_code'], var_name='year', value_name='electricityaccesspercent')\n",
    "\n",
    "    gdp_cleaned_df['year'] = gdp_cleaned_df['year'].astype('int')\n",
    "    population_cleaned_df['year'] = population_cleaned_df['year'].astype('int')\n",
    "    population_json_cleaned_df['year'] = population_json_cleaned_df['year'].astype('int')\n",
    "    population_sql_cleaned_df['year'] = population_sql_cleaned_df['year'].astype('int')\n",
    "    population_xml_cleaned_df['year'] = population_xml_cleaned_df['year'].astype('int')\n",
    "    rural_cleaned_df['year'] = rural_cleaned_df['year'].astype('int')\n",
    "    electricity_cleaned_df['year'] = electricity_cleaned_df['year'].astype('int')\n",
    "  \n",
    "    # Reset the index\n",
    "    population_cleaned_df = population_cleaned_df.reset_index(drop=True)\n",
    "    population_json_cleaned_df = population_json_cleaned_df.reset_index(drop=True)\n",
    "    population_sql_cleaned_df = population_sql_cleaned_df.reset_index(drop=True)\n",
    "    gdp_cleaned_df = gdp_cleaned_df.reset_index(drop=True)\n",
    "    rural_cleaned_df = rural_cleaned_df.reset_index(drop=True)\n",
    "    electricity_cleaned_df = electricity_cleaned_df.reset_index(drop=True)\n",
    "\n",
    "    projects_cleaned_df = projects_cleaned_df.drop_duplicates()\n",
    "    population_cleaned_df = population_cleaned_df.drop_duplicates()\n",
    "    population_json_cleaned_df = population_json_cleaned_df.drop_duplicates()\n",
    "    population_sql_cleaned_df = population_sql_cleaned_df.drop_duplicates()\n",
    "    population_xml_cleaned_df = population_xml_cleaned_df.drop_duplicates()\n",
    "    gdp_cleaned_df = gdp_cleaned_df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects_df_capped = projects_cleaned_df.copy()\n",
    "    population_sql_df_capped = population_sql_cleaned_df.copy()\n",
    "    population_df_capped = population_cleaned_df.copy()\n",
    "    population_json_df_capped = population_json_cleaned_df.copy()\n",
    "    population_xml_df_capped = population_xml_cleaned_df.copy()\n",
    "    gdp_df_capped = gdp_cleaned_df.copy()\n",
    "\n",
    "    # Apply Winsorizer to numeric columns in projects_df\n",
    "    for column in ['lendprojectcost', 'ibrdcommamt', 'idacommamt', 'totalamt']:\n",
    "        winsoriser_iqr = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=[column])\n",
    "        projects_df_capped[column] = winsoriser_iqr.fit_transform(projects_df_capped[[column]])\n",
    "\n",
    "    # Apply Gaussian Winsorizer to grantamt column in projects_df\n",
    "    winsoriser_gaussian = Winsorizer(capping_method='gaussian', tail='both', fold=2, variables=['grantamt'])\n",
    "    projects_df_capped['grantamt'] = winsoriser_gaussian.fit_transform(projects_df_capped[['grantamt']])\n",
    "\n",
    "    # Apply Winsorizer to numeric columns in population DataFrames\n",
    "    numeric_columns = population_df_capped.select_dtypes(include=['float64', 'int64']).columns\n",
    "    for column in numeric_columns:\n",
    "        winsoriser_iqr = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=[column])\n",
    "        population_df_capped[column] = winsoriser_iqr.fit_transform(population_df_capped[[column]])\n",
    "        population_json_df_capped[column] = winsoriser_iqr.fit_transform(population_json_df_capped[[column]])\n",
    "        population_sql_df_capped[column] = winsoriser_iqr.fit_transform(population_sql_df_capped[[column]]) \n",
    "        population_xml_df_capped[column] = winsoriser_iqr.fit_transform(population_xml_df_capped[[column]])\n",
    "\n",
    "    # Apply Winsorizer to gdp column in gdp_df\n",
    "    winsoriser_iqr_gdp = Winsorizer(capping_method='iqr', tail='both', fold=1.5, variables=['gdp'])\n",
    "    gdp_df_capped['gdp'] = winsoriser_iqr_gdp.fit_transform(gdp_df_capped[['gdp']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from datetime columns in projects_df\n",
    "    date_columns = ['boardapprovaldate', 'closingdate']\n",
    "    for col in date_columns:\n",
    "        projects_df_capped[col] = pd.to_datetime(projects_df_capped[col])\n",
    "        projects_df_capped[col + '_year'] = projects_df_capped[col].dt.year\n",
    "        projects_df_capped[col + '_month'] = projects_df_capped[col].dt.month\n",
    "        projects_df_capped[col + '_day'] = projects_df_capped[col].dt.day\n",
    "        projects_df_capped.drop([col], axis=1, inplace=True)\n",
    "\n",
    "    # Map 'Y' to 1 and 'N' to 0 in projects_df\n",
    "    projects_df_capped['supplementprojectflg'] = projects_df_capped['supplementprojectflg'].map({'Y': 1, 'N': 0})\n",
    "\n",
    "    # Encode categorical columns\n",
    "    projects_df_capped = encode_categorical_columns(projects_df_capped)\n",
    "    population_df_capped = encode_categorical_columns(population_df_capped)\n",
    "    population_sql_df_capped = encode_categorical_columns(population_sql_df_capped)\n",
    "    population_xml_df_capped = encode_categorical_columns(population_xml_df_capped)\n",
    "    population_json_df_capped = encode_categorical_columns(population_json_df_capped)\n",
    "    gdp_df_capped = encode_categorical_columns(gdp_df_capped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Scalling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define numerical columns for scaling\n",
    "    num_columns_projects = ['supplementprojectflg', 'lendprojectcost', 'ibrdcommamt', 'idacommamt', 'totalamt', 'grantamt', 'boardapprovaldate_year', 'boardapprovaldate_month', 'boardapprovaldate_day', 'closingdate_year', 'closingdate_month', 'closingdate_day']\n",
    "    num_columns_population = ['year', 'value']\n",
    "\n",
    "    # Scale numerical columns using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    projects_df_capped[num_columns_projects] = scaler.fit_transform(projects_df_capped[num_columns_projects]) \n",
    "    population_df_capped[num_columns_population] = scaler.fit_transform(population_df_capped[num_columns_population]) \n",
    "    population_sql_df_capped[num_columns_population] = scaler.fit_transform(population_sql_df_capped[num_columns_population]) \n",
    "    population_json_df_capped[num_columns_population] = scaler.fit_transform(population_json_df_capped[num_columns_population]) \n",
    "    population_xml_df_capped[num_columns_population] = scaler.fit_transform(population_xml_df_capped[num_columns_population]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    projects_cleaned_df = add_country_codes(projects_cleaned_df)\n",
    "    # Calculate project cost\n",
    "    projects_cleaned_df['project_cost'] = projects_cleaned_df['lendprojectcost'] + projects_cleaned_df['ibrdcommamt'] + projects_cleaned_df['idacommamt'] + projects_cleaned_df['totalamt'] + projects_cleaned_df['grantamt']\n",
    "    \n",
    "    # Group by 'country_code' and sum the project costs\n",
    "    country_project_cost = projects_cleaned_df.groupby('country_code')['project_cost'].sum().reset_index()\n",
    "    \n",
    "    # Merge population_cleaned_df and gdp_cleaned_df based on 'country_name', 'country_code', and 'year'\n",
    "    merged_cleaned_df = pd.merge(population_cleaned_df, gdp_cleaned_df, on=['country_name', 'country_code', 'year'], how='left', suffixes=('_pop', '_gdp'))\n",
    "    merged_cleaned_df = merged_cleaned_df.dropna()\n",
    "    \n",
    "    # Calculate GDP per capita and round it to 2 decimal places\n",
    "    merged_cleaned_df['gdp_per_capita'] = (merged_cleaned_df['gdp'] / merged_cleaned_df['value']).round(2)\n",
    "    \n",
    "    # Merge with country_project_cost using 'country_code' as the key\n",
    "    merged_cleaned_df = pd.merge(merged_cleaned_df, country_project_cost, on='country_code', how='left')\n",
    "    \n",
    "    # Join with rural_cleaned_df based on 'country_code' and 'year'\n",
    "    merged_cleaned_df = pd.merge(merged_cleaned_df, rural_cleaned_df[['country_code', 'year', 'ruralpopulationpercent']], on=['country_code', 'year'], how='left')\n",
    "    \n",
    "    # Join with electricity_cleaned_df based on 'country_code' and 'year'\n",
    "    merged_cleaned_df = pd.merge(merged_cleaned_df, electricity_cleaned_df[['country_code', 'year', 'electricityaccesspercent']], on=['country_code', 'year'], how='left')\n",
    "    \n",
    "    \n",
    "    # Fill NaN values in specific columns with 0\n",
    "    merged_cleaned_df['project_cost'].fillna(0, inplace=True)\n",
    "    merged_cleaned_df['ruralpopulationpercent'].fillna(0, inplace=True)\n",
    "    merged_cleaned_df['electricityaccesspercent'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Rename columns and add underscores\n",
    "    merged_cleaned_df.rename(columns={\n",
    "        'value': 'population',\n",
    "        'ruralpopulationpercent': 'rural_population_percent',\n",
    "        'electricityaccesspercent': 'electricity_access_percent'\n",
    "    }, inplace=True)\n",
    "    \n",
    "    data = {\n",
    "        'projects_cleaned_df': projects_cleaned_df,\n",
    "        'population_cleaned_df': population_cleaned_df,\n",
    "        'population_json_cleaned_df': population_json_cleaned_df,\n",
    "        'population_xml_cleaned_df': population_xml_cleaned_df,\n",
    "        'population_sql_cleaned_df': population_sql_cleaned_df,\n",
    "        'mystery_cleaned_df': mystery_cleaned_df,\n",
    "        'rural_cleaned_df': rural_cleaned_df,\n",
    "        'electricity_cleaned_df': electricity_cleaned_df,\n",
    "        'merged_cleaned_df': merged_cleaned_df,\n",
    "        'gdp_cleaned_df': gdp_cleaned_df\n",
    "            }\n",
    "    \n",
    "    # Define file paths\n",
    "    file_paths = {\n",
    "        'projects_cleaned_df': 'projects_cleaned.csv',\n",
    "        'population_cleaned_df': 'population_cleaned.csv',\n",
    "        'population_json_cleaned_df': 'population_json_cleaned.csv',\n",
    "        'population_xml_cleaned_df': 'population_xml_cleaned.csv',\n",
    "        'population_sql_cleaned_df': 'population_sql_cleaned.csv',\n",
    "        'mystery_cleaned_df': 'mystery_cleaned.csv',\n",
    "        'rural_cleaned_df': 'rural_cleaned.csv',\n",
    "        'electricity_cleaned_df': 'electricity_cleaned.csv',\n",
    "        'merged_cleaned_df': 'merged_cleaned.csv',\n",
    "        'gdp_cleaned_df': 'gdp_cleaned.csv'\n",
    "    }\n",
    "\n",
    "    # Save each cleaned DataFrame to a CSV file\n",
    "    for key, value in file_paths.items(): \n",
    "        file_path = '/opt/airflow/data/csv_result/' + value\n",
    "        data[key].to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the function to load DataFrame into SQLite\n",
    "def load_to_sqlite():\n",
    "    merged_cleaned_df = pd.read_csv('/opt/airflow/data/csv_result/merged_cleaned.csv')\n",
    "    # Connect to SQLite database (creates a new database if it doesn't exist)\n",
    "    conn = sqlite3.connect('/opt/airflow/data/db_result/merged_data.db')\n",
    "    \n",
    "    # Load merged_cleaned_df into the SQLite database\n",
    "    merged_cleaned_df.to_sql('merged_data', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Commit changes and close the connection\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Airflow Executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 5, 13),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'data_processing_dag',\n",
    "    default_args=default_args,\n",
    "    description='A DAG to process data',\n",
    "    schedule_interval=None,\n",
    ")\n",
    "\n",
    "\n",
    "# Specify your Google Cloud Storage bucket name and the source CSV file path \n",
    "source_object = '/opt/airflow/data/csv_result/merged_cleaned.csv'\n",
    "\n",
    "# Specify the destination bucket and object name in GCS\n",
    "destination_bucket = 'kmzway_dns_storage'\n",
    "destination_object = 'merged_cleaned.csv'\n",
    "\n",
    "# Specify the Google Cloud Storage connection ID\n",
    "google_cloud_storage_conn_id = 'google_cloud_default'\n",
    "\n",
    "load_data_task = PythonOperator(\n",
    "    task_id='load_data',\n",
    "    python_callable=process_data,\n",
    "    dag=dag,\n",
    "    provide_context=True\n",
    ")\n",
    "\n",
    "# Define the PythonOperator to execute the load_to_sqlite function\n",
    "load_to_sqlite_task = PythonOperator(\n",
    "    task_id='load_to_sqlite_task',\n",
    "    python_callable=load_to_sqlite,\n",
    "    dag=dag,\n",
    ")\n",
    " \n",
    "upload_gcs_task = LocalFilesystemToGCSOperator(\n",
    "        task_id='upload_to_gcs',\n",
    "        src=source_object,\n",
    "        dst=destination_object,\n",
    "        bucket=destination_bucket, \n",
    "        dag=dag,\n",
    "    )\n",
    "\n",
    "load_data_task >> load_to_sqlite_task >> upload_gcs_task\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
